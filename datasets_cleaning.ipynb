{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cJTqfqUfIg6k",
        "outputId": "10587f93-c86b-46c8-d6c9-d192d74f5d61"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processed sample_data/home_insurance.csv → cleaned_data/cleaned_home_insurance.csv\n",
            "Processed sample_data/car_co2.csv → cleaned_data/cleaned_car_co2.csv\n",
            "Processed sample_data/car_insurance.csv → cleaned_data/cleaned_car_insurance.csv\n",
            "Processed sample_data/synthetic_insurance_data.csv → cleaned_data/cleaned_synthetic_insurance_data.csv\n",
            "\n",
            "Batch processing complete. Check generated log file.\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import os\n",
        "from datetime import datetime\n",
        "from typing import List\n",
        "\n",
        "def process_insurance_files(file_paths: List[str]) -> None:\n",
        "    \"\"\"Process multiple insurance CSV files with full reporting.\"\"\"\n",
        "\n",
        "    # Create cleaned data directory\n",
        "    output_dir = \"cleaned_data\"\n",
        "    os.makedirs(output_dir, exist_ok=True)\n",
        "\n",
        "    # Configure logging\n",
        "    log_file = f\"data_cleaning_log_{datetime.now().strftime('%Y%m%d_%H%M%S')}.txt\"\n",
        "\n",
        "    with open(log_file, 'w') as log:\n",
        "        log.write(f\"Data Cleaning Report - {datetime.now()}\\n\\n\")\n",
        "\n",
        "        for file_path in file_paths:\n",
        "            try:\n",
        "                log.write(f\"{'='*50}\\n\")\n",
        "                log.write(f\"Processing: {file_path}\\n\")\n",
        "\n",
        "                # 1. Load data\n",
        "                raw_df = pd.read_csv(\n",
        "                    file_path,\n",
        "                    na_values=[\"NA\", \"N/A\", \"?\", \"Unknown\", \"\", \"-\", \"NaN\", \"null\"],\n",
        "                    engine='python',\n",
        "                    dtype={'Geo_Code': 'category'}  # Example type handling\n",
        "                )\n",
        "\n",
        "                # 2. Initial analysis\n",
        "                log.write(f\"\\nInitial Shape: {raw_df.shape}\\n\")\n",
        "                log.write(\"Missing Values:\\n\")\n",
        "                log.write(raw_df.isna().sum().to_markdown())\n",
        "                log.write(f\"\\nDuplicates: {raw_df.duplicated().sum()}\\n\")\n",
        "\n",
        "                # 3. Clean data\n",
        "                cleaned_df = raw_df.copy()\n",
        "\n",
        "                # Handle missing values\n",
        "                for col in cleaned_df.columns:\n",
        "                    if col == 'Customer Id':\n",
        "                        continue\n",
        "\n",
        "                    if pd.api.types.is_numeric_dtype(cleaned_df[col]):\n",
        "                        # Smart numeric imputation\n",
        "                        if cleaned_df[col].dtype == 'int64':\n",
        "                            fill_val = int(cleaned_df[col].median())\n",
        "                        else:\n",
        "                            fill_val = cleaned_df[col].median()\n",
        "                    else:\n",
        "                        fill_val = cleaned_df[col].mode()[0] if not cleaned_df[col].mode().empty else 'MISSING'\n",
        "\n",
        "                    cleaned_df[col] = cleaned_df[col].fillna(fill_val)\n",
        "\n",
        "                # Remove duplicates\n",
        "                initial_count = len(cleaned_df)\n",
        "                cleaned_df = cleaned_df.drop_duplicates()\n",
        "                removed_count = initial_count - len(cleaned_df)\n",
        "\n",
        "                # 4. Save results\n",
        "                base_name = os.path.basename(file_path)\n",
        "                output_path = os.path.join(output_dir, f\"cleaned_{base_name}\")\n",
        "                cleaned_df.to_csv(output_path, index=False)\n",
        "\n",
        "                # 5. Log results\n",
        "                log.write(f\"\\nCleaning Results:\\n\")\n",
        "                log.write(f\"- Removed duplicates: {removed_count}\\n\")\n",
        "                log.write(f\"- Final shape: {cleaned_df.shape}\\n\")\n",
        "                log.write(f\"- Output file: {output_path}\\n\")\n",
        "                log.write(\"Status: SUCCESS\\n\")\n",
        "\n",
        "                print(f\"Processed {file_path} → {output_path}\")\n",
        "\n",
        "            except Exception as e:\n",
        "                log.write(f\"\\nERROR processing {file_path}: {str(e)}\\n\")\n",
        "                log.write(\"Status: FAILED\\n\")\n",
        "                print(f\"Error processing {file_path}: {e}\")\n",
        "\n",
        "        log.write(\"\\nBatch processing complete\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    # List of files to process (update with your paths)\n",
        "    files_to_process = [\n",
        "        \"sample_data/home_insurance.csv\",\n",
        "        \"sample_data/car_co2.csv\",\n",
        "        \"sample_data/car_insurance.csv\",\n",
        "        \"sample_data/synthetic_insurance_data.csv\"\n",
        "    ]\n",
        "\n",
        "    process_insurance_files(files_to_process)\n",
        "    print(\"\\nBatch processing complete. Check generated log file.\")"
      ]
    }
  ]
}